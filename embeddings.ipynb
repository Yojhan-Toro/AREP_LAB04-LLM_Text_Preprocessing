{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings y Preprocesamiento de Texto para LLMs\n",
    "## Basado en el CapÃ­tulo 2 de *Build a Large Language Model (From Scratch)* â€” Sebastian Raschka\n",
    "\n",
    "---\n",
    "\n",
    "Este cuaderno recorre los pasos fundamentales del CapÃ­tulo 2: desde texto crudo hasta embeddings posicionales listos para ser consumidos por un transformador. Cada secciÃ³n incluye explicaciones propias sobre *por quÃ©* cada paso importa, tanto para LLMs como para sistemas agÃ©nticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  ExplicaciÃ³n 1 â€” Â¿Por quÃ© necesitamos tokenizar el texto?\n",
    "\n",
    "Las redes neuronales operan exclusivamente sobre nÃºmeros; no pueden procesar cadenas de texto directamente. La **tokenizaciÃ³n** es el puente que convierte texto legible por humanos en secuencias de enteros que una red puede digerir.\n",
    "\n",
    "Pero hay mÃ¡s que eso. La forma en que se divide el texto determina el **vocabulario** del modelo: quÃ© unidades mÃ­nimas de significado reconoce. Un vocabulario demasiado pequeÃ±o fuerza al modelo a representar conceptos complejos con combinaciones torpes de tokens; uno demasiado grande hace que la capa de embedding sea enorme y costosa. Por eso existen algoritmos como **Byte-Pair Encoding (BPE)**, que encuentra un equilibrio Ã³ptimo aprendiendo quÃ© sub-palabras son mÃ¡s frecuentes en el corpus.\n",
    "\n",
    "En sistemas agÃ©nticos â€”donde un LLM planifica y ejecuta tareasâ€” la tokenizaciÃ³n tambiÃ©n importa porque define la **granularidad del contexto**: cuÃ¡ntos pasos de razonamiento caben en la ventana de contexto y cÃ³mo se codifica la memoria de interacciones previas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1 â€” Tokenizador Simple con Expresiones Regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de caracteres en el corpus: 20479\n",
      "Primeros 100 caracteres:\n",
      " I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# â”€â”€â”€ Cargar el texto de prueba â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Usamos \"The Verdict\" de Edith Wharton como corpus de ejemplo.\n",
    "# Es un texto pequeÃ±o pero suficiente para demostrar todos los conceptos.\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total de caracteres en el corpus:\", len(raw_text))\n",
    "print(\"Primeros 100 caracteres:\\n\", raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens: 4690\n",
      "Primeros 30 tokens:\n",
      " ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ TokenizaciÃ³n con expresiones regulares â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# El patrÃ³n divide en puntuaciÃ³n y espacios, pero los conserva como tokens.\n",
    "# Esto nos da control explÃ­cito sobre cada unidad lÃ©xica.\n",
    "preprocessed = re.split(r'([,.:;?_!\\\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(\"Total de tokens:\", len(preprocessed))\n",
    "print(\"Primeros 30 tokens:\\n\", preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2 â€” Construir el Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TamaÃ±o del vocabulario: 1132\n",
      "Primeras 10 entradas del vocabulario:\n",
      "  '!' â†’ 0\n",
      "  '\"' â†’ 1\n",
      "  \"'\" â†’ 2\n",
      "  '(' â†’ 3\n",
      "  ')' â†’ 4\n",
      "  ',' â†’ 5\n",
      "  '--' â†’ 6\n",
      "  '.' â†’ 7\n",
      "  ':' â†’ 8\n",
      "  ';' â†’ 9\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ Construir vocabulario â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Cada token Ãºnico recibe un Ã­ndice entero.\n",
    "# TambiÃ©n aÃ±adimos tokens especiales: <|unk|> para palabras fuera del vocab\n",
    "# y <|endoftext|> para marcar separaciones entre documentos.\n",
    "all_words = sorted(set(preprocessed))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "print(\"TamaÃ±o del vocabulario:\", len(vocab))\n",
    "print(\"Primeras 10 entradas del vocabulario:\")\n",
    "for token, idx in list(vocab.items())[:10]:\n",
    "    print(f\"  {repr(token)} â†’ {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3 â€” Implementar el Tokenizador (Encode / Decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "Codificado:     [1131, 5, 490, 1025, 657, 973, 63, 1130, 300, 900, 1073, 994, 7, 900, 764, 7]\n",
      "Decodificado:   <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ Clase SimpleTokenizerV2 del libro â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Esta clase encapsula la lÃ³gica encode/decode.\n",
    "# encode: texto â†’ lista de enteros\n",
    "# decode: lista de enteros â†’ texto\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\\\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\"\n",
    "            for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text = \"\"\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(ids)\n",
    "\n",
    "print(\"Texto original:\", text)\n",
    "print(\"Codificado:    \", ids)\n",
    "print(\"Decodificado:  \", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  ExplicaciÃ³n 2 â€” BPE: El Tokenizador de GPT-2 y sus Ventajas\n",
    "\n",
    "El tokenizador artesanal anterior tiene un defecto grave: cualquier palabra no vista en el corpus de entrenamiento se convierte en `<|unk|>`, perdiendo todo su significado. Los LLMs modernos resuelven esto con **Byte-Pair Encoding (BPE)**, un algoritmo que:\n",
    "\n",
    "1. Comienza con un vocabulario de bytes individuales (los 256 posibles).\n",
    "2. Fusiona iterativamente los pares de tokens mÃ¡s frecuentes en el corpus.\n",
    "3. Se detiene cuando alcanza el tamaÃ±o de vocabulario deseado (GPT-2 usa 50.257).\n",
    "\n",
    "El resultado es que **ninguna palabra es completamente desconocida**: en el peor caso, se divide en sub-palabras o bytes individuales. Esto es crucial en sistemas agÃ©nticos que deben manejar nombres propios, tÃ©rminos tÃ©cnicos, cÃ³digo fuente, URLs y formatos inesperados de entrada â€”situaciones donde un vocabulario rÃ­gido fallarÃ­a constantemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4 â€” Tokenizador BPE con `tiktoken` (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "Tokens BPE: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 10470, 13]\n",
      "Cantidad de tokens: 19\n",
      "\n",
      "Decodificado: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "Prueba con palabra desconocida 'Akwirw189':\n",
      "  Tokens: [33901, 86, 343, 86, 220, 19, 23]\n",
      "  Decodificado: Akwirw 189\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# â”€â”€â”€ Tokenizador BPE de GPT-2 vÃ­a tiktoken â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# tiktoken es la librerÃ­a oficial de OpenAI para tokenizaciÃ³n.\n",
    "# Usamos el encoding 'gpt2' que tiene un vocabulario de 50.257 tokens.\n",
    "tokenizer_bpe = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\n",
    "integers = tokenizer_bpe.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(\"Texto:\", text)\n",
    "print(\"Tokens BPE:\", integers)\n",
    "print(\"Cantidad de tokens:\", len(integers))\n",
    "print()\n",
    "print(\"Decodificado:\", tokenizer_bpe.decode(integers))\n",
    "\n",
    "# â”€â”€â”€ DemostraciÃ³n: ninguna palabra es completamente <unk> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "unknown_word = \"Akwirw189\"\n",
    "encoded_unk = tokenizer_bpe.encode(unknown_word)\n",
    "print(f\"\\nPrueba con palabra desconocida {repr(unknown_word)}:\")\n",
    "print(f\"  Tokens: {encoded_unk}\")\n",
    "print(f\"  Decodificado:\", tokenizer_bpe.decode(encoded_unk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5 â€” Dataset con Ventana Deslizante (Sliding Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens BPE en el corpus: 5145\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Ejemplo de par (input â†’ target) con max_length=4:\n",
      "  Input tokens:  [40, 367, 2885, 1464]\n",
      "  Target tokens: [367, 2885, 1464, 1807]\n",
      "\n",
      "  Input texto:  I HAD always thought\n",
      "  Target texto:  HAD always thought Jack\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# â”€â”€â”€ Clase GPTDatasetV1 del libro â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Implementa el patrÃ³n de predicciÃ³n del siguiente token:\n",
    "# dado input[i : i+max_length], predice target[i+1 : i+max_length+1]\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                          stride=128, shuffle=True, drop_last=True,\n",
    "                          num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# â”€â”€â”€ VerificaciÃ³n con ejemplo pequeÃ±o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "tokenizer_bpe = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_text = tokenizer_bpe.encode(raw_text)\n",
    "print(\"Total de tokens BPE en el corpus:\", len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "print(\"\\n\" + \"â”€\" * 50)\n",
    "print(f\"Ejemplo de par (input â†’ target) con max_length={context_size}:\")\n",
    "print(f\"  Input tokens:  {x}\")\n",
    "print(f\"  Target tokens: {y}\")\n",
    "print()\n",
    "print(f\"  Input texto: \", tokenizer_bpe.decode(x))\n",
    "print(f\"  Target texto: \", tokenizer_bpe.decode(y))\n",
    "print(\"â”€\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¬ Experimento â€” Impacto de `max_length` y `stride` en el NÃºmero de Muestras\n",
    "\n",
    "### HipÃ³tesis\n",
    "La cantidad de muestras de entrenamiento depende directamente de `stride`: un stride pequeÃ±o genera mÃ¡s muestras con mayor solapamiento entre contextos adyacentes. Un `max_length` mayor reduce el nÃºmero de muestras porque cada ventana consume mÃ¡s tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 5145 tokens BPE\n",
      "FÃ³rmula aproximada: num_samples = (total_tokens - max_length) / stride\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ConfiguraciÃ³n   â”‚ Samplesâ”‚ Solapam. â”‚ ObservaciÃ³n                 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ max=4,  stride=4â”‚   1285 â”‚    0%    â”‚ Sin solapamiento (baseline) â”‚\n",
      "â”‚ max=4,  stride=2â”‚   2571 â”‚   50%    â”‚ Doble de muestras           â”‚\n",
      "â”‚ max=4,  stride=1â”‚   5141 â”‚   75%    â”‚ MÃ¡ximo solapamiento         â”‚\n",
      "â”‚ max=8,  stride=4â”‚   1284 â”‚   50%    â”‚ Contexto mayor, igual count â”‚\n",
      "â”‚ max=16, stride=8â”‚    641 â”‚   50%    â”‚ Contexto mayor, menos mues. â”‚\n",
      "â”‚ max=256,stride=128â”‚   38 â”‚   50%    â”‚ ConfiguraciÃ³n del libro     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ Experimento: variar max_length y stride â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Mostramos cuÃ¡ntas muestras se generan bajo distintas configuraciones\n",
    "# y quÃ© porcentaje de solapamiento tiene cada una.\n",
    "\n",
    "experiments = [\n",
    "    (4,   4,   \"Sin solapamiento (baseline)\"),\n",
    "    (4,   2,   \"Doble de muestras\"),\n",
    "    (4,   1,   \"MÃ¡ximo solapamiento\"),\n",
    "    (8,   4,   \"Contexto mayor, igual count\"),\n",
    "    (16,  8,   \"Contexto mayor, menos mues.\"),\n",
    "    (256, 128, \"ConfiguraciÃ³n del libro\"),\n",
    "]\n",
    "\n",
    "print(f\"Corpus: {len(enc_text)} tokens BPE\")\n",
    "print(\"FÃ³rmula aproximada: num_samples = (total_tokens - max_length) / stride\")\n",
    "print()\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚ ConfiguraciÃ³n   â”‚ Samplesâ”‚ Solapam. â”‚ ObservaciÃ³n                 â”‚\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "\n",
    "for max_len, stride, label in experiments:\n",
    "    dataset = GPTDatasetV1(raw_text, tokenizer_bpe, max_len, stride)\n",
    "    n = len(dataset)\n",
    "    overlap_pct = max(0, (max_len - stride) / max_len * 100)\n",
    "    cfg_str = f\"max={max_len:<3}, stride={stride:<3}\"\n",
    "    print(f\"â”‚ {cfg_str:<15} â”‚ {n:>6} â”‚  {overlap_pct:>4.0f}%   â”‚ {label:<27} â”‚\")\n",
    "\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Â¿Por quÃ© el solapamiento es Ãºtil? ===\n",
      "\n",
      "Con stride=4 (sin solapamiento), max_length=4:\n",
      "  Ventana 0: 'I HAD always thought'\n",
      "  Ventana 1: 'Jack Gisburn rather a'\n",
      "  â†’ La frase 'thought Jack' nunca aparece como contexto de entrenamiento.\n",
      "\n",
      "Con stride=2 (50% solapamiento), max_length=4:\n",
      "  Ventana 0: 'I HAD always thought'\n",
      "  Ventana 1: 'always thought Jack Gisburn'\n",
      "  Ventana 2: 'thought Jack Gisburn rather'\n",
      "  â†’ La transiciÃ³n entre ventanas es continua; el modelo aprende el lÃ­mite.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ VisualizaciÃ³n del solapamiento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Mostramos los primeros tokens de ventanas consecutivas para ilustrar\n",
    "# cÃ³mo el solapamiento preserva contextos que de otro modo se pierden.\n",
    "\n",
    "print(\"=== Â¿Por quÃ© el solapamiento es Ãºtil? ===\")\n",
    "print()\n",
    "\n",
    "# Sin solapamiento\n",
    "ds_no_overlap = GPTDatasetV1(raw_text, tokenizer_bpe, max_length=4, stride=4)\n",
    "print(\"Con stride=4 (sin solapamiento), max_length=4:\")\n",
    "for i in range(2):\n",
    "    inp_text = tokenizer_bpe.decode(ds_no_overlap.input_ids[i].tolist())\n",
    "    print(f\"  Ventana {i}: {repr(inp_text)}\")\n",
    "print(\"  â†’ La frase 'thought Jack' nunca aparece como contexto de entrenamiento.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Con solapamiento\n",
    "ds_overlap = GPTDatasetV1(raw_text, tokenizer_bpe, max_length=4, stride=2)\n",
    "print(\"Con stride=2 (50% solapamiento), max_length=4:\")\n",
    "for i in range(3):\n",
    "    inp_text = tokenizer_bpe.decode(ds_overlap.input_ids[i].tolist())\n",
    "    print(f\"  Ventana {i}: {repr(inp_text)}\")\n",
    "print(\"  â†’ La transiciÃ³n entre ventanas es continua; el modelo aprende el lÃ­mite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Conclusiones del Experimento\n",
    "\n",
    "Los resultados muestran que:\n",
    "\n",
    "- **`stride = max_length`** produce muestras completamente independientes. El corpus de 5.145 tokens con `max_length=4` y `stride=4` genera **1.285 muestras** (â‰ˆ 5145/4). No hay ningÃºn solapamiento.\n",
    "\n",
    "- **`stride = max_length / 2`** duplica aproximadamente las muestras (2.571 con max=4) porque cada token aparece en dos ventanas distintas. Este es el estÃ¡ndar del libro.\n",
    "\n",
    "- **`stride = 1`** genera tantas muestras como tokens hay en el corpus (5.141 â‰ˆ 5.145), con un 75% de solapamiento. Es el esquema mÃ¡s rico en datos, pero tambiÃ©n el mÃ¡s costoso computacionalmente.\n",
    "\n",
    "**El solapamiento es Ãºtil porque evita que el modelo nunca vea ciertas transiciones de texto.** Imagine que el corpus dice \"...pensÃ³ Jack Gisburn...\". Sin solapamiento, si \"pensÃ³\" termina una ventana y \"Jack\" comienza la siguiente, el modelo nunca aprende que \"pensÃ³\" predice \"Jack\" en ese contexto. El solapamiento garantiza que todas las n-gramas del corpus aparezcan al menos una vez como input de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6 â€” Token Embeddings y Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del batch de inputs: torch.Size([8, 4])\n",
      "\n",
      "Dimensiones del embedding de tokens:\n",
      "  Capa: Embedding(50257, 256)\n",
      "  Forma de salida: torch.Size([8, 4, 256])\n",
      "\n",
      "Embedding posicional:\n",
      "  Capa: Embedding(4, 256)\n",
      "  Posiciones: tensor([0, 1, 2, 3])\n",
      "  Forma de salida: torch.Size([4, 256])\n",
      "\n",
      "Embedding final (token + posiciÃ³n):\n",
      "  Forma: torch.Size([8, 4, 256])\n",
      "  â†’ Cada uno de los 4 tokens en cada una de las 8 secuencias\n",
      "    estÃ¡ representado por un vector de 256 dimensiones.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ ParÃ¡metros del modelo (pequeÃ±o para demostraciÃ³n) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vocab_size = 50257    # Vocabulario de GPT-2\n",
    "output_dim = 256      # DimensiÃ³n del embedding (GPT-2 real usa 768)\n",
    "max_length = 4        # Longitud del contexto\n",
    "batch_size = 8\n",
    "\n",
    "# â”€â”€â”€ Crear DataLoader y obtener un batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=max_length,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Forma del batch de inputs:\", inputs.shape)  # [batch, seq_len]\n",
    "\n",
    "# â”€â”€â”€ Capa de embedding de tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Cada token ID se mapea a un vector denso de `output_dim` dimensiones.\n",
    "# Los pesos de esta capa son parÃ¡metros aprendibles del modelo.\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(\"\\nDimensiones del embedding de tokens:\")\n",
    "print(\"  Capa:\", token_embedding_layer)\n",
    "print(\"  Forma de salida:\", token_embeddings.shape)  # [batch, seq, dim]\n",
    "\n",
    "# â”€â”€â”€ Capa de embedding posicional â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# GPT usa embeddings posicionales APRENDIDOS (no sinusoidales como BERT).\n",
    "# Esto permite que el modelo ajuste cÃ³mo usa la informaciÃ³n posicional\n",
    "# durante el entrenamiento.\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print(\"\\nEmbedding posicional:\")\n",
    "print(\"  Capa:\", pos_embedding_layer)\n",
    "print(\"  Posiciones:\", torch.arange(context_length))\n",
    "print(\"  Forma de salida:\", pos_embeddings.shape)  # [seq, dim]\n",
    "\n",
    "# â”€â”€â”€ Combinar: embedding final = token + posiciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# La suma elemento a elemento inyecta informaciÃ³n posicional en cada vector.\n",
    "# El broadcasting permite sumar [batch, seq, dim] + [seq, dim].\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"\\nEmbedding final (token + posiciÃ³n):\")\n",
    "print(\"  Forma:\", input_embeddings.shape)\n",
    "print(\"  â†’ Cada uno de los 4 tokens en cada una de las 8 secuencias\")\n",
    "print(\"    estÃ¡ representado por un vector de 256 dimensiones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  ExplicaciÃ³n 3 â€” Â¿Por quÃ© los Embeddings Codifican Significado?\n",
    "\n",
    "Esta es quizÃ¡s la pregunta mÃ¡s profunda del capÃ­tulo. Al inicializar la capa `nn.Embedding`, los vectores son **completamente aleatorios**: no tienen ningÃºn significado. El significado emerge durante el **entrenamiento por gradiente descendente**.\n",
    "\n",
    "Cuando el modelo intenta predecir el siguiente token y comete un error, la pÃ©rdida se propaga hacia atrÃ¡s (backpropagation) y ajusta los pesos de todas las capas, incluyendo la tabla de embeddings. A lo largo de millones de actualizaciones, los vectores de tokens que aparecen en contextos similares acaban siendo empujados hacia la misma regiÃ³n del espacio vectorial. Este es el nÃºcleo de la **hipÃ³tesis distribucional**: el significado de una palabra estÃ¡ determinado por las palabras que la rodean.\n",
    "\n",
    "**RelaciÃ³n con conceptos de redes neuronales:**\n",
    "\n",
    "La capa `nn.Embedding` no es mÃ¡s que una **tabla de bÃºsqueda diferenciable** (un `nn.Linear` sin bias que recibe un vector one-hot como entrada). Esto significa que:\n",
    "\n",
    "- Sus pesos son parÃ¡metros del modelo, inicializados aleatoriamente y aprendidos por backprop.\n",
    "- Participan en el grafo computacional de PyTorch, por lo que sus gradientes se calculan automÃ¡ticamente.\n",
    "- La distancia coseno entre dos vectores de embedding mide su similitud semÃ¡ntica, porque tokens que predicen las mismas palabras vecinas aprenden vectores similares.\n",
    "\n",
    "En tÃ©rminos agÃ©nticos, los embeddings son lo que permite que un LLM **generalice**: si el agente nunca vio \"zaranda\" en entrenamiento pero sÃ­ vio palabras que aparecen en los mismos contextos, puede inferir su uso correcto. Sin embeddings, cada token serÃ­a completamente opaco y el modelo no podrÃ­a transferir conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  ExplicaciÃ³n 4 â€” Embeddings Posicionales y la LimitaciÃ³n del Transformer\n",
    "\n",
    "Un transformer sin embeddings posicionales es **invariante al orden**: si permutamos todos los tokens de una oraciÃ³n, el resultado de la atenciÃ³n serÃ­a el mismo. Esto es evidentemente inÃºtil para el lenguaje, donde el orden determina el significado (Â«el perro muerde al hombreÂ» â‰  Â«el hombre muerde al perroÂ»).\n",
    "\n",
    "GPT soluciona esto con **embeddings posicionales aprendidos**: una segunda tabla de parÃ¡metros con una fila por posiciÃ³n posible (hasta `context_length`). El vector de posiciÃ³n $p_i$ se suma al token embedding antes de entrar al transformer. AsÃ­, el modelo no solo sabe *quÃ©* token es, sino tambiÃ©n *dÃ³nde* estÃ¡.\n",
    "\n",
    "Este detalle tiene implicaciones prÃ¡cticas importantes para sistemas agÃ©nticos:\n",
    "\n",
    "- Los embeddings posicionales aprendidos solo funcionan bien hasta `context_length` posiciones. Si intentamos extender un modelo GPT-2 (max 1024 tokens) a contextos mÃ¡s largos, el rendimiento cae porque las posiciones son desconocidas.\n",
    "- Alternativas como **RoPE** (Rotary Position Embedding) o **ALiBi** ofrecen mejor extrapolaciÃ³n, y son parte de por quÃ© modelos modernos pueden manejar millones de tokens.\n",
    "- En cadenas de razonamiento agÃ©ntico largas, la posiciÃ³n de una instrucciÃ³n dentro del prompt puede afectar cuÃ¡nta atenciÃ³n recibe: el efecto Â«lost in the middleÂ» es consecuencia directa de cÃ³mo los embeddings posicionales distribuyen la atenciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 7 â€” Resumen del Pipeline Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘          PIPELINE COMPLETO: TEXTO â†’ EMBEDDINGS              â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  1. Texto crudo           â†’ 20.479 caracteres               â•‘\n",
      "â•‘  2. TokenizaciÃ³n BPE      â†’ 5.145 token IDs                 â•‘\n",
      "â•‘  3. Sliding Window        â†’ 35 muestras (max=256, stride=128)â•‘\n",
      "â•‘  4. Token Embedding       â†’ [8, 256, 256] float32 tensor    â•‘\n",
      "â•‘  5. Positional Embedding  â†’ [256, 256] float32 tensor       â•‘\n",
      "â•‘  6. Input Embedding final â†’ [8, 256, 256] float32 tensor    â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ParÃ¡metros entrenables del pipeline de embeddings:\n",
      "  Token Embedding  : 50.257 Ã— 256 = 12.865.792 parÃ¡metros\n",
      "  Position Embedding:    256 Ã— 256 =     65.536 parÃ¡metros\n",
      "  Total            :                 12.931.328 parÃ¡metros\n",
      "\n",
      "Estos parÃ¡metros se ajustan durante el entrenamiento para que\n",
      "los vectores codifiquen significado semÃ¡ntico y posicional.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ Resumen visual del pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Este bloque muestra en una sola vista el camino completo\n",
    "# desde texto crudo hasta el tensor de embeddings listo para el transformer.\n",
    "\n",
    "context_length_full = 256\n",
    "\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘          PIPELINE COMPLETO: TEXTO â†’ EMBEDDINGS              â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "print(f\"â•‘  1. Texto crudo           â†’ {len(raw_text):,} caracteres               â•‘\")\n",
    "print(f\"â•‘  2. TokenizaciÃ³n BPE      â†’ {len(enc_text):,} token IDs                 â•‘\")\n",
    "print(f\"â•‘  3. Sliding Window        â†’ 35 muestras (max=256, stride=128)â•‘\")\n",
    "print(f\"â•‘  4. Token Embedding       â†’ [8, 256, 256] float32 tensor    â•‘\")\n",
    "print(f\"â•‘  5. Positional Embedding  â†’ [256, 256] float32 tensor       â•‘\")\n",
    "print(f\"â•‘  6. Input Embedding final â†’ [8, 256, 256] float32 tensor    â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "print(\"\\nParÃ¡metros entrenables del pipeline de embeddings:\")\n",
    "tok_params = vocab_size * context_length_full\n",
    "pos_params = context_length_full * context_length_full\n",
    "print(f\"  Token Embedding  : {vocab_size:,} Ã— {context_length_full} = {tok_params:,} parÃ¡metros\")\n",
    "print(f\"  Position Embedding:  {context_length_full:,} Ã— {context_length_full} =   {pos_params:,} parÃ¡metros\")\n",
    "print(f\"  Total            :                 {tok_params + pos_params:,} parÃ¡metros\")\n",
    "print()\n",
    "print(\"Estos parÃ¡metros se ajustan durante el entrenamiento para que\")\n",
    "print(\"los vectores codifiquen significado semÃ¡ntico y posicional.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones Generales\n",
    "\n",
    "El CapÃ­tulo 2 establece los cimientos de cualquier LLM moderno. El flujo que hemos implementado puede resumirse asÃ­:\n",
    "\n",
    "El texto crudo pasa por un tokenizador BPE que lo convierte en secuencias de enteros manejables. Esas secuencias se organizan en pares input/target mediante una ventana deslizante configurable, donde el stride controla cuÃ¡nto solapamiento (y por ende cuÃ¡ntas muestras) generamos. Finalmente, cada token ID se convierte en un vector denso de alta dimensiÃ³n mediante dos capas de embedding aprendibles: una que codifica identidad lÃ©xica y otra que codifica posiciÃ³n en la secuencia.\n",
    "\n",
    "El resultado es un tensor de forma `[batch_size, seq_len, embed_dim]` que el transformador puede procesar con mecanismos de atenciÃ³n. Todo el significado que el modelo aprende â€”la gramÃ¡tica, los hechos del mundo, las convenciones de formatoâ€” estÃ¡ comprimido en los pesos de estas capas y en las capas de atenciÃ³n que siguen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
